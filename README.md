# ğŸ§  Neural Network from Scratch (NumPy, No TF/Keras)

## ğŸ“Œ Overview

This project implements a simple neural network from scratch using only NumPy and pure mathematics â€” without TensorFlow, PyTorch, or Keras.
The network is trained on the MNIST handwritten digit dataset (0â€“9) to demonstrate forward propagation, backpropagation, and gradient descent.
The goal is to understand the internals of a neural network by building every component step by step.

## âœ¨ Features

- âš¡ Forward and backward propagation implemented manually
- ğŸ”¢ Supports ReLU and Softmax activation functions
- ğŸŒ€ Gradient descent optimization without external ML libraries
- ğŸ§© One-hot encoding for labels
- ğŸ–¼ï¸ Trains on the MNIST dataset
- ğŸ“ˆ Prints training accuracy during training

## ğŸ“‚ Repository Structure

ğŸ“¦ NeuralNetwork_fromScratch/
â”£ ğŸ“œ basic.py # Core implementation of the network
â”£ ğŸ“œ README.md # Documentation
â”— ğŸ“œ requirements.txt # Dependencies (numpy, matplotlib, pandas)

## âš™ï¸ Installation

Clone the repository and install dependencies:
git clone <https://github.com/LeenaKannan/NeuralNetwork_fromScratch.git>
cd NeuralNetwork_fromScratch
pip install -r requirements.txt

## Train the model

W1, b1, W2, b2 = gradient_descent(X_train, Y_train, alpha=0.1, iterations=500)

## ğŸ”‘ Code Explanation (Core Functions)

- ğŸ—ï¸ `init_params()` â†’ Initializes weights and biases randomly.
- ğŸ’¡ `ReLU(Z)` â†’ ReLU activation: replaces negatives with 0.
- ğŸ¯ `softmax(Z)` â†’ Converts raw scores into probabilities.
- ğŸ”„ `forward_prop(...)` â†’ Performs forward propagation.
- ğŸ§® `ReLU_deriv(Z)` â†’ Derivative of ReLU for backprop.
- ğŸ·ï¸ `one_hot(Y)` â†’ Converts labels into one-hot vectors.
- ğŸ“¤ `backward_prop(...)` â†’ Computes gradients with backprop.
- ğŸ”§ `update_params(...)` â†’ Updates weights/biases (gradient descent).
- ğŸ“Š `get_predictions(...)` â†’ Returns predicted class labels.
- âœ… `get_accuracy(...)` â†’ Computes prediction accuracy.
- ğŸ” `gradient_descent(...)` â†’ Full training loop.

## ğŸ§® Mathematical Formulation

### ğŸ”€ Forward Propagation

$$Z^{[1]} = W^{[1]}X + b^{[1]}, \quad A^{[1]} = ReLU(Z^{[1]})$$

$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}, \quad A^{[2]} = softmax(Z^{[2]})$$

### ğŸ”„ Backward Propagation

$$dZ^{[2]} = A^{[2]} - Y, \quad dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T}, \quad db^{[2]} = \frac{1}{m}\sum dZ^{[2]}$$

$$dZ^{[1]} = W^{[2]T}dZ^{[2]} \odot ReLU'(Z^{[1]}), \quad dW^{[1]} = \frac{1}{m}dZ^{[1]}X^T, \quad db^{[1]} = \frac{1}{m}\sum dZ^{[1]}$$

### âš™ï¸ Gradient Descent Update

$$W^{[l]} = W^{[l]} - \alpha dW^{[l]}, \quad b^{[l]} = b^{[l]} - \alpha db^{[l]}$$

## ğŸ“Š Training Setup (MNIST)

- ğŸ–¼ï¸ Input size: 784 (28Ã—28 pixels flattened)
- ğŸ—ï¸ Hidden layer: 10 neurons with ReLU
- ğŸ¯ Output layer: 10 neurons with Softmax (digits 0â€“9)
- âš¡ Optimizer: Gradient Descent

## ğŸ¤ Contributing

Ideas for improvement:

- â• Add more hidden layers
- âš¡ Try advanced optimizers (Adam, RMSProp)
- ğŸ“‰ Plot accuracy/loss curves
- ğŸ” Add gradient checking tests

## ğŸ“œ License

ğŸ“„ This project is licensed under the MIT License.
